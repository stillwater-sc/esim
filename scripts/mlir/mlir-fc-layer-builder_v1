import numpy as np
from mlir.ir import *
from mlir.dialects.func import *
from mlir.dialects.tosa import *
import mlir.execution_engine
from mlir.passmanager import PassManager


def create_fc_layer_ir(input_size=10, output_size=5):
    # Create the context and load dialects
    context = Context()
    # The dialects are automatically registered when we import them

    # Create random weights and biases
    weights = np.random.randn(input_size, output_size).astype(np.float32)
    biases = np.random.randn(output_size).astype(np.float32)

    # Create module
    with context, Location.unknown():
        module = Module.create()

        # Create function type
        input_type = RankedTensorType.get([1, input_size], F32Type.get())
        output_type = RankedTensorType.get([1, output_size], F32Type.get())
        fn_type = FunctionType.get([input_type], [output_type])

        # Create the function
        with InsertionPoint(module.body):
            @FuncOp.from_py_func(input_type)
            def fc_relu(arg):
                # Create constant for weights
                weight_type = RankedTensorType.get([input_size, output_size], F32Type.get())
                weight_attr = DenseElementsAttr.get(weights, type=weight_type)
                #weight_const = ConstOp(weight_type, weight_attr)
                weight_const = ConstOp(weight_attr).result

                # Create constant for biases
                bias_type = RankedTensorType.get([output_size], F32Type.get())
                bias_attr = DenseElementsAttr.get(biases, type=bias_type)
                #bias_const = ConstOp(bias_type, bias_attr)
                bias_const = ConstOp(bias_attr).result

                # Create zero constant for ReLU
                zero_type = RankedTensorType.get([1, output_size], F32Type.get())
                zero_attr = DenseElementsAttr.get_splat(zero_type, FloatAttr.get(F32Type.get(), 0.0))
                #zero_const = ConstOp(zero_type, zero_attr)
                zero_const = ConstOp(zero_attr).result

                # Reshape bias for broadcasting
                bias_shape_type = RankedTensorType.get([2], IntegerType.get_signless(64))
                bias_shape = [1, output_size]
                bias_shape_attr = DenseElementsAttr.get(np.array(bias_shape, dtype=np.int64))
                # reshaped_bias = ReshapeOp(
                #     RankedTensorType.get([1, output_size], F32Type.get()),
                #     bias_const,
                #     bias_shape_attr
                # )
                reshaped_bias = ReshapeOp(bias_const, bias_shape_attr).result

                # Matmul operation
                # matmul = MatMulOp(
                #     RankedTensorType.get([1, output_size], F32Type.get()),
                #     arg,
                #     weight_const
                # )
                matmul = MatMulOp(arg, weight_const).result

                # Add biases
                # add = AddOp(
                #     RankedTensorType.get([1, output_size], F32Type.get()),
                #     matmul,
                #     reshaped_bias
                # )
                add = AddOp(matmul, reshaped_bias).result

                # ReLU activation
                # relu = MaximumOp(
                #     RankedTensorType.get([1, output_size], F32Type.get()),
                #     add,
                #     zero_const
                # )
                relu = MaximumOp(add, zero_const).result

                return relu

    return module


def save_mlir_to_file(module, filename="fc_layer.mlir"):
    with open(filename, "w") as f:
        f.write(str(module))
    print(f"MLIR module has been written to {filename}")


if __name__ == "__main__":
    # Create the MLIR module
    module = create_fc_layer_ir(10, 5)

    # Save to file
    save_mlir_to_file(module)
