import numpy as np
from mlir.ir import *
from mlir.dialects.func import *
from mlir.dialects.tosa import *
import mlir.execution_engine
from mlir.passmanager import PassManager

def create_fc_layer_ir(input_size=10, output_size=5):
    context = Context()
    
    # Create random weights and biases
    weights = np.random.randn(input_size, output_size).astype(np.float32)
    biases = np.random.randn(output_size).astype(np.float32)
    
    with context, Location.unknown():
        module = Module.create()
        
        # Create function type
        input_type = RankedTensorType.get([1, input_size], F32Type.get())
        output_type = RankedTensorType.get([1, output_size], F32Type.get())
        fn_type = FunctionType.get([input_type], [output_type])
        
        with InsertionPoint(module.body):
            @FuncOp.from_py_func(input_type)
            def fc_relu(arg):
                # Create constant for weights
                weight_type = RankedTensorType.get([input_size, output_size], F32Type.get())
                weight_attr = DenseElementsAttr.get(weights, type=weight_type)
                weight_const = ConstOp.create(result_types=[weight_type], 
                                            attributes={'value': weight_attr}).result
                
                # Create constant for biases
                bias_type = RankedTensorType.get([output_size], F32Type.get())
                bias_attr = DenseElementsAttr.get(biases, type=bias_type)
                bias_const = ConstOp.create(result_types=[bias_type],
                                          attributes={'value': bias_attr}).result
                
                # Create zero constant for ReLU
                zero_type = RankedTensorType.get([1, output_size], F32Type.get())
                zero_attr = DenseElementsAttr.get_splat(zero_type, FloatAttr.get(F32Type.get(), 0.0))
                zero_const = ConstOp.create(result_types=[zero_type],
                                          attributes={'value': zero_attr}).result
                
                # Reshape bias for broadcasting
                reshape_out_type = RankedTensorType.get([1, output_size], F32Type.get())
                bias_shape = np.array([1, output_size], dtype=np.int64)
                bias_shape_attr = DenseElementsAttr.get(bias_shape)
                reshaped_bias = ReshapeOp.create(
                    result_types=[reshape_out_type],
                    operands=[bias_const],
                    attributes={'new_shape': bias_shape_attr}).result
                
                # Matmul operation
                matmul_out_type = RankedTensorType.get([1, output_size], F32Type.get())
                matmul = MatMulOp.create(
                    result_types=[matmul_out_type],
                    operands=[arg, weight_const]).result
                
                # Add biases
                add_out_type = RankedTensorType.get([1, output_size], F32Type.get())
                add = AddOp.create(
                    result_types=[add_out_type],
                    operands=[matmul, reshaped_bias]).result
                
                # ReLU activation
                relu_out_type = RankedTensorType.get([1, output_size], F32Type.get())
                relu = MaximumOp.create(
                    result_types=[relu_out_type],
                    operands=[add, zero_const]).result
                
                return relu

    return module

def save_mlir_to_file(module, filename="fc_layer.mlir"):
    with open(filename, "w") as f:
        f.write(str(module))
    print(f"MLIR module has been written to {filename}")

if __name__ == "__main__":
    # Create the MLIR module
    module = create_fc_layer_ir(10, 5)
    
    # Save to file
    save_mlir_to_file(module)
